{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents <a id='top'></a>\n",
    "\n",
    "1. <a href=#intro>Introduction</a>\n",
    "1. <a href=#pre-proc>NLP Pre-processing</a>\n",
    "1. <a href=#bow>Bag of Words Model</a>\n",
    "1. <a href=#lda>Topic Modeling</a>\n",
    "1. <a href=#senti>Sentiment Analysis</a>\n",
    "1. <a href=#ref>References and Links</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# 1. Introduction\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "In our world, Natural Language Processing (NLP) is used in several scenarios. For example, \n",
    "\n",
    "    * phones and handheld computers support predictive text and handwriting recognition; \n",
    "    * web search engines give access to information locked up in unstructured text; \n",
    "    * machine translation allows us to understand texts written in languages that we do not know; \n",
    "    * text analysis enables us to detect sentiment in tweets and blogs.\n",
    "\n",
    "But as we begin to explore Natural Language, we realise that it is an extremely difficult subject. Here are some specific points to note:\n",
    "\n",
    "1. Some words mean different things in different contexts, but us humans know which meaning is being used.\n",
    "    * He **served** the **dish**.\n",
    "1. In the following two sentences, the word \"by\" has different meanings:\n",
    "    * The lost children were found by the lake.\n",
    "    * The lost children were found by the search party.\n",
    "1. In the following cases, we (humans) can resolve what \"they\" is referring to, but it is not easy to generate a simple rule that a computer can follow.\n",
    "    * The thieves stole the paintings. They were subsequently recovered.\n",
    "    * The thieves stole the paintings. They were subsequently arrested.\n",
    "1. How can we get a computer to understand the following tweet?:\n",
    "    * \"Wow. Great job st@rbuck's. Best cup of coffee ever.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Before we go on, it would be useful to establish some terminology.\n",
    "\n",
    "A *document* is a string of text. It could contain an essay on a particular topic, a paragraph, a single sentence, or even just a tweet of 140 characters. \n",
    "\n",
    "A *text corpus* is a large collection of text documents. The plural of corpus is *corpora*. \n",
    "\n",
    "Each document is typically split into *tokens* before the actual NLP is done. Consider the sentence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I am watching television."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical *tokenisation* would be "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'I',  'am',  'watching', 'television', '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we tokenize and pre-process things will affect our final results. We shall discuss this more in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of NLP\n",
    "\n",
    "Corpora could come in different formats and have different structures.\n",
    "\n",
    "![text-corpus-structure](../figs/text-corpus-structure_nlp.png)\n",
    "\n",
    "Sometimes, the format or relationships between the documents in a corpus restrict or direct us toward certain types of analyses. In general, the goals of NLP can be varied. Here are some of the use-cases that we shall discuss:\n",
    "\n",
    "1. *Computing similarities between new documents and existing ones in a corpus*. This is an unsupervised approach. If the new document can be considered a \"query\", then this can be used to retrieve and prioritise documents that are relevant to the query.\n",
    "2. *Identifying \"topics\" within a corpus*. This is also an unsupervised technique, and allows us to identify the salient topics of a new document automatically. This could be useful in a customer feedback setting, because it would allow quick allocation or prioritisation of resources. This approach requires one to decide on the number of topics. It typically also requires some study of the topics in order to interpret and verify them.\n",
    "3. *Identifying the overarching sentiment of a document*. Using a lexicon of words and their tagged sentiments, we can assess whether the sentiment in a document is mostly positive or negative.\n",
    "\n",
    "There are some other applications and ideas that we may not have time to cover:\n",
    "\n",
    "1. *Named Entity Recognition* If we can identify the particular real-world entities that a query is referring to, we would be able to provide more accurate answers from the corpus. This concept is utilised in chatbots.\n",
    "2. *Word Embeddings*. This procedure provides a vector representation of a word, in the context of its semantic usage. With this, we can study the context of a word in this corpus, and understand what it means here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Reviews Dataset\n",
    "\n",
    "A dataset containing wine reviews is accessible from [Kaggle](https://www.kaggle.com/zynicide/wine-reviews). We shall work with one of the csv files. It contains 130,000 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = pd.read_csv(\"../data/winemag-data-130k-v2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(wine_reviews.duplicated(subset=None, keep='last') == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(wine_reviews.duplicated(subset=None, keep='first') == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews[wine_reviews.description == wine_reviews.description[2408]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(wine_reviews.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.points.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.description.values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"descriptions\" column contains the review for a particular wine by a user, whose name and twitter handle are provided. Also included is information such as the price, originating county, region of the wine, and so on. In our activity, we are going to apply NLP techniques to the wine reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre-proc'></a>\n",
    "# 2. NLP Pre-Processing\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Text documents consist of sentences of varying lengths. Usually, the first step to analysing a document is to break it up into pieces. This process is known as **tokenizing**. When tokenizing a document, we can do it at several levels of resolution: at the sentence, line, word or even punctuation level.\n",
    "\n",
    "For today, we are going to to simply tokenize each document by whitespace. This is easily done using `.split()` within built-in Python. But after that, we need to further pre-process the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import *\n",
    "from gensim import models, corpora, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, compact=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gensim` package includes a module for pre-processing text strings. Here is a list of some of the functions there:\n",
    "\n",
    "* `strip_multiple_whitespaces`\n",
    "* `strip_non_alphanum`\n",
    "* `strip_numeric`\n",
    "* `strip_punctuation`\n",
    "* `strip_short`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = wine_reviews.description.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(r1.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(strip_punctuation(r1.lower()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'This contains a number 144.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(strip_numeric(sent1.lower()).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since what we are about to do in the initial part of our activity is based on frequency counts of tokens, apart from some of the above steps, we are also going to remove common \"filler\" words that could end up skewing the eventual probability distributions of counts. These filler words are known as *stop words*. Linguists came up with them. They vary from model to model, from Python package to package, and of course, from language to language.\n",
    "\n",
    "It depends on your particular application if stop-word removal is meaningful or not. At times, it is only done in order to speed up the training of a model. However, it is possible to change the entire meaning of a sentence by removing stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'I do not like eating ice cream.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(s1).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.pprint(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us, we are going to apply this list of filters to each review:\n",
    "\n",
    "1. strip_punctuation(),\n",
    "2. strip_multiple_whitespaces(),\n",
    "3. strip_numeric(),\n",
    "4. remove_stopwords(),\n",
    "5. strip_short(),\n",
    "6. lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing a word is to reduce it to its root word. You will come across stemming whenever you read about lemmatizing. In both cases, we wish to reduce a word to its root word so that we do not have to deal with multiple variations of a token, such as ate, eating, and eats.\n",
    "\n",
    "When we stem a word, the prefix and/or suffix will be removed according to a set of rules. Since it is primarily rule-based, the resulting word may not be an actual English word.\n",
    "\n",
    "Like stemming, lemmatizing also aims to reduce a word to its root form. However, it differs from stemming in that the final word must be a proper English language word. For this purpose, the algorithm has to be supplied with a lexicon or dictionary, along with the text to be lemmatized.\n",
    "\n",
    "Here is an example that demonstrates the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "porter = PorterStemmer()\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "demo_sentence = 'Cats and ponies have a meeting'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[porter.stem(x) for x in demo_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('wordnet')\n",
    "[wn.lemmatize(x) for x in demo_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us go ahead and perform the pre-processing on the wine reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_FILTER = [lambda x: x.lower(), strip_punctuation, \n",
    "                 strip_multiple_whitespaces, strip_numeric, \n",
    "                 remove_stopwords, strip_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review_strings = wine_reviews.description.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strings_tokenized = [preprocess_string(x, CUSTOM_FILTER) for x in all_review_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(all_strings_tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in time, what we have is a list of lists. Each sub-list contains the tokens for a particular wine_review. For instance, the original review for row 234 was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(wine_reviews.description.values[233])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(all_strings_tokenized[233])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` does not have a lemmatizer, so we use the nltk lemmatizer on each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_corpus = [[wn.lemmatize(w) for w in dd ] for dd in all_strings_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(preprocessed_corpus[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(all_strings_tokenized[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bow'></a>\n",
    "# 3. Tfidf Model (Bag-of-Words)\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "In order to perform machine learning on text documents, we have to convert the text content into numerical features. The first method we shall study is to use the Bag of Words (BoW) representation. Here's how it is done, starting from a corpus:\n",
    "\n",
    "1. Build a vocabulary of words from the corpus and assign a fixed integer id to each word.\n",
    "2. For each document $i$ in the collection, count the number of occurrences of word $j$ in the dictionary.\n",
    "3. Store this count in the $(i,j)$-th entry of a matrix.\n",
    "\n",
    "At this point we have a numerical representation of the corpus. Let us take a look at a simple example instead of the wine review one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "            \"So basic that they won’t be very interesting , I’m afraid. \", \n",
    "            \"\"\"The point of these basic examples is to learn how basic text \n",
    "            counting works on *very simple* data, so that we are not afraid when \n",
    "            it comes to larger text documents. The sentences are here just to provide words.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the default pre-processing filters from gensim and inspect the tokens now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = preprocess_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(example_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what proceeds, we build a vocabulary of terms in the corpus, and convert them to the Bag of Words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = corpora.Dictionary(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(list(d1.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.token2id['basic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.id2token[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bow = [d1.doc2bow(x) for x in example_corpus]\n",
    "pp.pprint(example_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a sparse representation of each document in our corpus. We can in fact convert it back to the processed corpus mode at any time.\n",
    "\n",
    "The counts indicate the number of times each feature (or words) were present in the document. As you might observe, longer documents tend to contain larger counts (see document 3, which has many more 1's and even a couple of 2's. Thus, instead of dealing with counts, \n",
    "we shall convert each row into a vector of length 1. Words that appear in all documents will be weighted down by this transformation, since these do not help to distinguish the document from others. This transformation is known as the TF-IDF transformation.\n",
    "\n",
    "Instead of the raw counts, we define:\n",
    "\n",
    "* $N$ to be the number of documents ($N=3$ in the little example above).\n",
    "* $tf_{i,j}$ to be the frequency of term $i$ in document $j$.\n",
    "* $df_{i}$ to be the frequency of term $i$ across all documents.\n",
    "* $w'_{i,j}$ to be: \n",
    "\n",
    "\\begin{equation}\n",
    "w'_{i,j} = tf_{i,j} \\times \\log_2 \\left( \\frac{N}{df_{i}} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Then the final $w_{i,j}$ for term $i$ in document $j$ is the normalised version of $w'_{i,j}$ across the terms in that document. \n",
    "\n",
    "Consider the word \"sentences\", in document id 02 (the third document).\n",
    "* $N = 3$\n",
    "* $tf_{i,j} = 1$\n",
    "* $df_{i} = 2$\n",
    "\n",
    "Thus\n",
    "\n",
    "\\begin{equation}\n",
    "w'_{ij} = 1 \\times \\log_2 ( 3/2) = 0.58\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tfidf = models.TfidfModel(example_bow, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(example_tfidf[example_bow[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(example_bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(example_tfidf[d1.doc2bow(['basic', 'afraid', 'data'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us move back to the wine reviews and create the tfidf transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = corpora.Dictionary(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dct.items())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct.dfs[31830]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(119988/31325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dct.num_docs\n",
    "bow_corpus = [dct.doc2bow(text) for text in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(preprocessed_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(dictionary=dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(tfidf[dct.doc2bow(preprocessed_corpus[0])])\n",
    "#pp.pprint(tfidf[bow_corpus[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can find matching documents, we build a similarity index, so that matches are returned quicker. We try something simple at first:\n",
    "\n",
    "> Which documents/reviews are similar to the first one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.Similarity(None, corpus=tfidf[bow_corpus], num_features=len(dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[tfidf[bow_corpus[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most similar reviews to review id 0. Of course, the first review itself is there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(-sims)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(wine_reviews.description[[0, 108537, 2000, 9929]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[[0, 108537, 2000, 9929], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sims = index[tfidf[bow_corpus[20]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try an arbitrary query. Suppose we are interested in \"fruity full palate\" wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'acidic oaky chardonnay'.split()\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[tfidf[dct.doc2bow(q1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_results = np.argsort(-sims)[:10]\n",
    "q1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(wine_reviews.description.values[q1_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[q1_results, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda'></a>\n",
    "# 4. Topic Modeling\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "The LDA (Latent Dirichlet Allocation) model assumes the following intuitive generative process for the documents:\n",
    "\n",
    "1. There is a set of $K$ topics that the documents come from. Each document contains words from several topics. There is a probability mass function on the topics for each document.\n",
    "2. For each topic, there is a probability mass function for the distribution of words in that topic. \n",
    "\n",
    "At the end of LDA topic modeling, we will be able to tell, for a particular (new or old) document: the weight combination of the topics for that document. For each topic, we would be able to tell the terms that are salient.\n",
    "\n",
    "LDA only gives us the probabilistic weights &mdash; we have to interpret them ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda1 = models.LdaModel(corpus=bow_corpus, num_topics=20, id2word=dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp.pprint(lda1.get_topic_terms(19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint([dct.id2token[x] for x in [290, 581, 167, 851, 131, 100, 1041, 473, 115, 54]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp.pprint(lda1.show_topics(num_topics = 20, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(lda1.get_document_topics(bow_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(preprocessed_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is topic distribution for a particular document, e.g.\n",
    "# output of lda1.get_document_topics(bow_corpus[0])\n",
    "#\n",
    "# output is True if that document has more than threshold probability for topic_num\n",
    "def check_for_topic(topic_pmf, topic_num, threshold):\n",
    "    for topic, prob in topic_pmf:\n",
    "        if (topic == topic_num) & (prob > threshold):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_list = [check_for_topic(lda1.get_document_topics(x), 3, 0.8) for x in bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.description.values[np.where(tf_list)[0][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(lda1.get_document_topics(bow_corpus[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda1.get_topic_terms(13, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dct.id2token[x] for x in [16, 2, 58, 54, 38, 41, 254, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint([dct.id2token[x] for x in [125, 114, 10, 3, 0, 2, 16, 133]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda1.log_perplexity(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh = CoherenceModel(model=lda1, corpus=bow_corpus, coherence='u_mass')\n",
    "\n",
    "coh.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda1.print_topic(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda1.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda1, bow_corpus, dictionary=lda1.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis\n",
    "pyLDAvis.save_html(vis, 'vis.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='senti'></a>\n",
    "# 5. Sentiment Analysis\n",
    "<a href=#top>(back to top)</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentiText, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss.polarity_scores('I really hate this class!! :-(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.polarity_scores('I really hate this class!! :-D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['sentiment'] = wine_reviews.description.apply(lambda x: ss.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.sentiment.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(wine_reviews[wine_reviews.sentiment < -0.5].sample(10).description.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# 6. References\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "1. [nltk](https://www.nltk.org/) This site contains the documentation for the nltk and a book on NLP. The book is a little on the old side (no deep-learning based techniques) but still very useful.\n",
    "2. [gensim documentation](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "3. There are a few DataCamp courses on NLP that you can continue with when you have time. Here are their titles:\n",
    "    * Introduction to Natural Language Processing in Python.\n",
    "    * Advanced NLP with spaCy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
